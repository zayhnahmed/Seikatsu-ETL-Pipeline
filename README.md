ğŸŒ± # Seikatsu ETL Pipeline

A minimal and modular **Extractâ€“Transformâ€“Load (ETL) pipeline** designed to preprocess journal data for analytics and AI-driven insights in the **Seikatsu** personal development application.

This repository is a **standalone ETL demonstration**, intentionally decoupled from frontend and application-specific logic, to clearly showcase data engineering fundamentals.

---

## ğŸ“Œ Overview

The Seikatsu ETL Pipeline processes raw user journal entries through three well-defined stages:

1. **Extract** â€“ Ingest raw journal data from a JSON source  
2. **Transform** â€“ Clean, normalize, and enrich textual data  
3. **Load** â€“ Persist structured data for downstream analytics  

The pipeline forms the foundation for behavioral analysis, trend detection, and future AI-based recommendation systems.

---

## ğŸ§© ETL Pipeline Architecture

```
Raw Journal Data (JSON)
          â†“
      Extract
          â†“
     Transform
(cleaning, normalization)
          â†“
       Load
 (structured output)
```

---

## ğŸ“‚ Project Structure

```
seikatsu-etl-pipeline/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw_journals.json        # Sample raw journal entries
â”‚   â””â”€â”€ processed_journals.json  # Output after ETL
â”‚
â”œâ”€â”€ etl/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ extract.py               # Data extraction logic
â”‚   â”œâ”€â”€ transform.py             # Data cleaning & enrichment
â”‚   â””â”€â”€ load.py                  # Data persistence & summaries
â”‚
â”œâ”€â”€ main.py                      # Pipeline orchestrator
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

---

## ğŸ”„ Pipeline Stages

### 1ï¸âƒ£ Extract
- Reads raw journal entries from a JSON file
- Handles file I/O and parsing errors
- Outputs structured Python objects
- Simulates ingestion of user-generated content

---

### 2ï¸âƒ£ Transform
- Trims whitespace and normalizes text
- Converts timestamps to ISO format
- Calculates word count metrics
- Extracts mood and category fields
- Produces a clean, consistent data schema

---

### 3ï¸âƒ£ Load
- Writes processed data to a JSON storage layer
- Generates summary statistics
- Displays category-wise distribution
- Designed to be easily extended to PostgreSQL or vector databases

---

## â–¶ï¸ How to Run

### Clone the repository
```bash
git clone https://github.com/<your-username>/seikatsu-etl-pipeline
cd seikatsu-etl-pipeline
```

### Install dependencies
```bash
pip install -r requirements.txt
```

### Run the complete pipeline
```bash
python main.py
```

---

## ğŸ§ª Run Individual Stages (Optional)

```bash
# Test extraction
python etl/extract.py

# Test transformation
python etl/transform.py

# Test loading
python etl/load.py
```

---

## ğŸ“Š Sample Output

```
============================================================
SEIKATSU ETL PIPELINE
============================================================

[1/3] EXTRACTING raw journal data...
âœ“ Extracted 5 journal entries

[2/3] TRANSFORMING and cleaning data...
âœ“ Transformed 5 journal entries

[3/3] LOADING processed data...
âœ“ Loaded 5 journal entries to data/processed_journals.json

Summary Statistics:
  Total entries: 5
  Total words: 67
  Avg words/entry: 13.4

Category Distribution:
  Career: 1
  Learning: 1
  Relationships: 1
  Spirituality: 1
  Strength: 1

============================================================
âœ“ ETL PIPELINE COMPLETED SUCCESSFULLY
============================================================
```

---

## ğŸ›  Technical Stack

- **Language**: Python 3.8+
- **Dependencies**: Python Standard Library only
- **Data Format**: JSON
- **Architecture**: Modular ETL design

---

## ğŸ¯ Use Case in Seikatsu

This ETL pipeline serves as the data preprocessing layer for:

- User journal analytics
- Behavioral trend analysis
- AI-powered insights and recommendations
- Data consistency and quality enforcement
- Future retrieval-augmented (RAG-ready) systems

---

## ğŸš€ Future Enhancements

- PostgreSQL integration for persistent storage
- pgvector support for semantic search
- Batch processing for large-scale data
- Data validation using Pydantic
- Workflow orchestration with Airflow or Prefect

---

## ğŸ“ Academic Context

This project demonstrates:

- Clear separation of ETL concerns
- Real-world data engineering practices
- Scalable and extensible pipeline design
- Readiness for AI and analytics workloads

---

## ğŸ“ Note

This repository is intentionally minimal to highlight core ETL concepts in a clean, understandable, and reviewer-friendly manner.
